{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cookie cutter notebook is intended to help guide the process from a city-specific rain gage csv format, to a generic NetCDF containing timeSeries data and with attributes explaining how the file was created. For best results, make a copy of this notebook and delete any cells that you don't need. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roadmap\n",
    " 1. [Inspect file](#1.-Inspect-file)\n",
    " 2. [Read data into pandas.Dataframe object](#2.-Read-data-into-pandas.Dataframe-object)\n",
    " 3. [Read locations into pandas.Dataframe object](#3.-Read-locations-into-pandas.Dataframe-object)\n",
    " 4. [Figure out how these two Dataframes overlap](#4.-Figure-out-how-these-two-Dataframes-overlap)\n",
    " 5. [Write a function that reads all the data files](#5.-Write-a-function-that-reads-all-the-data-files)\n",
    " 6. [QAQC](#6.-QAQC)\n",
    "     - [missing loc](#missing-loc) \n",
    "     - [TZ check](#TZ-check)\n",
    "     - [bad values](#bad-values)\n",
    "     - [units check](#units-check)\n",
    " 7. [Write to NetCDF](#7.-Write-to-NetCDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Inspect file\n",
    "If there is going to be something strange in the data you can bet it'll be in the beginning or end. So we will look at those first to try to spot stupid stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head -4 /home/jsignell/data/BALTIMORE/BaltoCounty/8-31-2015.xlsx.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!tail -4 /home/jsignell/data/BALTIMORE/BaltoCounty/8-31-2015.xlsx.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read data into pandas.Dataframe object\n",
    "This is probably the most iterative step. By the end you want a time parsed and indexed dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = '/home/jsignell/data/BALTIMORE/BaltoCounty/8-31-2015.xlsx.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gage = pd.read_csv(f, parse_dates=['Time'], index_col='Time', skipfooter=1, engine='python')\n",
    "gage.index.name = 'time'\n",
    "gage.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read locations into pandas.Dataframe object\n",
    "The locations can be wonky, but the files are small and locations matter a lot, so don't skimp on the data wrangling at this stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = '/home/jsignell/data/BALTIMORE/Location_names.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "locs = pd.read_csv(f, delim_whitespace=True, header=None,\n",
    "                   skiprows=45, skipfooter=1, usecols=[2,3,4],\n",
    "                   engine='python', index_col=0)\n",
    "locs.index.name='station'\n",
    "locs.columns = ['lon', 'lat']\n",
    "locs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After aquiring this file, we were given another file with some different stations and some overlap. Let's see how different the files really are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1 = '/home/jsignell/data/BALTIMORE/County_BES_rain_gages.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "locs1 = pd.read_excel(f1, index_col=0)\n",
    "locs1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we want to know if there are gages in locs1 that aren't in locs, and we want to know if the gages that are in both are in the same locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(locs1.index) - set(locs.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compare = locs1.join(locs, how='inner')\n",
    "compare[(compare.Latitude != compare.lat.round(4)) | (compare.Longitude != compare.lon.round(4))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like for our purposes locs includes more precise location data, so that is probably the better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Figure out how these two Dataframes overlap\n",
    "We will first look at the names of the gages in both dataframes, and then see what kind of parsing is going to be needed to get matching gage names. In the case below, the units (in) are included in the variable names in the gage data, but not in the locations. So we will need to strip this part of each string off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gage.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "locs.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col = gage.columns[0]\n",
    "col.split('-')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = [col.split('-')[0].split('.')[0] for col in gage.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have the locations for these gages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(columns) - set(locs.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can still keep moving forward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gage.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gage.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Write a function that reads all the data files\n",
    "Now that we have a sense of what we are looking at and how the gage files and the location files overlap, we can write a function that we will proceeed to use on each of the files. Use whatever you learned above about the structure of the files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = '/home/jsignell/data/BALTIMORE/BaltoCounty/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def func(f):\n",
    "    # read file in using the default style\n",
    "    gage = pd.read_csv(f, parse_dates=['Time'], index_col='Time')\n",
    "    \n",
    "    # check that date time has been parsed and if it hasn't try to fix it\n",
    "    if gage.index.dtype != '<M8[ns]':\n",
    "        gage = pd.read_csv(f, parse_dates=['Time'], index_col='Time',\n",
    "                           skipfooter=1, engine='python')\n",
    "    \n",
    "    # name index time\n",
    "    gage.index.name = 'time'\n",
    "    \n",
    "    # set gage_names to match those in the locs dataframe\n",
    "    #gage.columns = [col.split('-')[0].split('.')[0] for col in gage.columns]\n",
    "    columns = []\n",
    "    for col in gage.columns:\n",
    "        col = col.split('-')[0].split('.')[0]\n",
    "        if col.endswith('RG'):\n",
    "            col = col[:-2]\n",
    "        columns.append(col)\n",
    "    gage.columns = columns\n",
    "\n",
    "    return gage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_list = []\n",
    "for f in files:\n",
    "    df_list.append(func(path+f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.concat(df_list)\n",
    "df = df.sort_index()\n",
    "\n",
    "df.columns.name = 'station'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. QAQC\n",
    "[missing loc](#missing-loc) | [TZ check](#TZ-check) | [bad values](#bad-values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bad values\n",
    "This is probably the most decision based QAQC check, but sometimes it is very easy to toss out values because they are ridiculous. This is the case when there are values that exceed the upper extents of the possible. We won't try to figure out what these values should be. We will just set them to NAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.max().sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.plot(df['HR18'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.plot(df['GF10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.plot(df['BC53'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.loc['2009-08-10':'2009-08-29','HR18'] = pd.np.nan\n",
    "df.loc['2013-05-14':'2013-05-18','GF10'] = pd.np.nan\n",
    "df.loc['2016-05-17':'2016-05-18','BC53'] = pd.np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### missing loc\n",
    "We can visually represent data availability, and see which gages aren't listed in our locs dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(df.columns) - set(locs.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.pcolormesh(df.isnull().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "There are almost always the same number of gages, but their locations or possibly just their names change at a certaion point in time. We can't resolve this by being clever or guessing. We need to go back to the source and figure it out. Or we need to provide masked locations for the gages that don't have locations listed. That way if we figure things out at a later date, we can just add that info in rather than coming back to the beginning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### External Input:\n",
    "We were told that gages that end in A are at the same locations as their A-less peers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_columns(df, cols):\n",
    "    '''Combine given columns in a dataframe and retain name from first column'''\n",
    "    bar = pd.concat([df[col] for col in cols])\n",
    "    df[cols[0]] = bar.dropna().sort_index().drop_duplicates()\n",
    "    return df.drop(cols[1:], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = combine_columns(df, ['HR16', 'HR16/HR16A', 'HR16A'])\n",
    "df = combine_columns(df, ['BC40', 'BC40A'])\n",
    "df = combine_columns(df, ['BC43', 'BC43A'])\n",
    "df = combine_columns(df, ['BC50', 'BC50A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(df.columns) - set(locs.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TZ check\n",
    "Time zone is very important and not often carried around in the metadata. If it is, that is awesome, and we can use it now. It is always best to ask around and see if you can figure out what you have. Guessing timezones bases on data is a tricky and time consuming business. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gage.index = gage.index.tz_localize('US/Eastern').tz_convert('UTC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't know the time zone - as is often the case - we can try to find a storm and use radar data to make a best guess. Normally there are three viable options for timezones: UTC, local standard time, and local time. By this I mean that people can either take or leave the daylight savings part. The storm should be in what would be daylight savings time (winter) so that we can be sure to test whether daylight savings is used or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.mean(axis=1).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in order to get a sense of this storm we will compare the rain gage data from the storm with the radar data. So we can save the storm off into a csv, and then load it in the TZ checking notebook running in in the radar environment. This allows us to download all the radar data from the storm, calculate rainfall, pull out rainfall at the gage locations, and then resample and take the mean to get the average 5min rain rate over the gages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "locs.join(df['2015-11-19 10:00':'2015-11-19 20:00'].T, how='inner').to_csv('storm_gage.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<img src=tmp/UTC_2015-11-19.png/ width=400/>\n",
    "<img src=tmp/US_Eastern_2015-11-19.png/ width=400/>\n",
    "<img src=tmp/EST_2015-11-19.png/ width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to check your timezone naming options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pytz import all_timezones\n",
    "from pytz import common_timezones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like EST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set timezone and convert to UTC\n",
    "df.index = df.index.tz_localize('EST').tz_convert('UTC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### units check\n",
    "We just know it is in inches, so let's convert to mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df*=25.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Write to NetCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/jsignell/erddapData/UrbanRainfall/Baltimore/'\n",
    "RGN='Baltimore_County'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "site = 'Baltimore County'\n",
    "\n",
    "units = 'mm'\n",
    "tz = 'storm_guessed'\n",
    "calc_from_tips = False\n",
    "label = 'right'\n",
    "freq = '5min'\n",
    "per_hour = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.index = df.index.astype('datetime64[ns]')\n",
    "\n",
    "datasets = [xr.DataArray(df[col]) for col in df.columns]\n",
    "combined = xr.concat(datasets, 'station')\n",
    "ds0 = combined.to_dataset(name='rain_gage')\n",
    "ds0['rain_gage'].attrs.update({'units': units, 'standard_name': 'gage rain depth',\n",
    "                               'label': label, 'freq': freq, 'per_hour': per_hour,\n",
    "                               'tz': tz, 'calc_from_tips': calc_from_tips})\n",
    "#ds0['rain_gage'].encoding.update({'chunksizes': (5, 10000), 'zlib': True})\n",
    "ds0['station'] = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we only care about locations where we have gage data\n",
    "s = df.mean(axis=0)\n",
    "s.name = 'historical_mean'\n",
    "locs = locs.join(s, how='inner')[['lon', 'lat']]\n",
    "\n",
    "ds1 = xr.Dataset.from_dataframe(locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds_ = ds0.merge(ds1)\n",
    "ds_.set_coords(['lon', 'lat'], inplace=True)\n",
    "ds_['station'] = ds_['station'].astype(str)\n",
    "ds_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds_.station.attrs.update({'standard_name': 'station_name', 'long_name': 'station name', 'cf_role':'timeseries_id'})\n",
    "ds_.lat.attrs.update({'standard_name': 'latitude', 'long_name':'station latitude', 'units': 'degrees_north'})\n",
    "ds_.lon.attrs.update({'standard_name': 'longitude', 'long_name':'station longitude', 'units': 'degrees_east'})\n",
    "ds_.time.encoding = {'units':'seconds since 1970-01-01', 'calendar':'gregorian', 'dtype': pd.np.double}\n",
    "#                     'chunksizes': (10000,), 'zlib': True}\n",
    "\n",
    "ds_.attrs.update({'description': '{site} rain gage network'.format(site=site),\n",
    "                  'history': 'Created {now}'.format(now=pd.datetime.now()),\n",
    "                  'Conventions': \"CF-1.6\",\n",
    "                  'featureType': 'timeSeries'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for year in range(pd.Timestamp(ds_.time[0].values).year,\n",
    "                  pd.Timestamp(ds_.time[-1].values).year+1):\n",
    "    if not os.path.isdir(DATA_PATH + str(year)):\n",
    "        os.mkdir(DATA_PATH + str(year))\n",
    "    ds_.sel(time=str(year)).to_netcdf('{path}{year}/{RGN}_freq.nc'.format(\n",
    "            path=DATA_PATH, year=year, RGN=RGN), format='netCDF4', engine='h5netcdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
